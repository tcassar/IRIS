# Presentation

## Slide one
We use th hardware to gather audio and visual data about the surroundings.
Today we trained a model to listen for our wake word, "ok iris". This is performed on device. 
We stream photo and audio via web sockets to a server.
We use large langauge models to generate a description of the scene. I will discuss this later.
    
Now onto Tom.

## Slide two

We used Google Cloud services, and OpenAI, to process images and audio in the cloud
Focusing on the visual processing

we first used google vision api to analyse a photo of the scene 
